# Sample Databricks Setup

This folder contains example configurations and notebooks for running `dblstreamgen` in Databricks.

## ğŸ“ Folder Structure

```
sample/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config_generation.yaml   # Event schemas
â”‚   â””â”€â”€ config_source_kinesis.yaml  # Kinesis config (with Databricks secrets)
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_install_and_test.py   # Installation & quick test
    â”œâ”€â”€ 02_stream_generator.py   # StreamGenerator example
    â””â”€â”€ 03_batch_generator.py    # BatchGenerator example (future)
```

## ğŸš€ Quick Start in Databricks

### Step 1: Create Unity Catalog Volume (Recommended)

```sql
-- Create a volume for libraries
CREATE VOLUME IF NOT EXISTS <catalog>.<schema>.libraries
COMMENT 'Shared libraries and wheels';

-- Create a volume for configs
CREATE VOLUME IF NOT EXISTS <catalog>.<schema>.config
COMMENT 'Configuration files';
```

### Step 2: Upload the Wheel

1. Build the wheel (if not already built):
   ```bash
   cd /path/to/dblstreamgen
   python -m build
   ```

2. Upload to Unity Catalog Volume:
   - Option A: Use Databricks CLI
     ```bash
     databricks fs cp dist/dblstreamgen-0.1.0-py3-none-any.whl \
       /Volumes/<catalog>/<schema>/libraries/dblstreamgen-0.1.0-py3-none-any.whl
     ```
   - Option B: Use Workspace Repos (for development)
   - Option C: Upload via UI â†’ Catalog â†’ Volumes â†’ libraries â†’ Upload

### Step 3: Upload Configs

Upload the `configs/` folder to Unity Catalog Volume:
```bash
databricks fs cp configs/config_generation.yaml \
  /Volumes/<catalog>/<schema>/config/config_generation.yaml
databricks fs cp configs/config_source_kinesis.yaml \
  /Volumes/<catalog>/<schema>/config/config_source_kinesis.yaml
```

**Important**: Edit `config_source_kinesis.yaml` and add your AWS credentials using Databricks secrets!

### Step 4: Create Databricks Secrets (Recommended)

```bash
# Create secret scope
databricks secrets create-scope --scope dblstreamgen

# Add AWS credentials
databricks secrets put --scope dblstreamgen --key aws-access-key
databricks secrets put --scope dblstreamgen --key aws-secret-key
```

Then in `config_source_kinesis.yaml`:
```yaml
kinesis_config:
  aws_access_key_id: "{{secrets/dblstreamgen/aws-access-key}}"
  aws_secret_access_key: "{{secrets/dblstreamgen/aws-secret-key}}"
```

### Step 5: Run Notebooks

1. Import notebooks from `notebooks/` folder
2. Update paths to use your Unity Catalog volume paths
3. Run `01_install_and_test.py` first to verify installation
4. Then run `02_stream_generator.py` to generate events

### Step 4: Run Notebooks

1. Import notebooks from `notebooks/` folder
2. Run `01_install_and_test.py` first to verify installation
3. Then run `02_stream_generator.py` to generate events

## ğŸ“ Notes

- This `sample/` folder is in `.gitignore` - customize freely for your environment
- Update AWS credentials and Kinesis stream names in configs
- StreamGenerator works in any Databricks notebook
- BatchGenerator requires a Spark cluster (coming soon)

## ğŸ” Security Best Practices

**Never commit credentials!**

Use Databricks secrets:
```python
# In notebooks
dbutils.secrets.get(scope="dblstreamgen", key="aws-access-key")
```

Or update config to reference secrets:
```yaml
aws_access_key_id: "{{secrets/dblstreamgen/aws-access-key}}"
```

The library will automatically resolve `{{secrets/...}}` references.

